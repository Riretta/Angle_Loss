{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import import_ipynb\n",
    "import adamw.cyclic_scheduler as CyclicLRWithRestarts\n",
    "import adamw.adamw as AdamW\n",
    "import Caps_basics.CapsNet_Layers_multiFC as CapsNet_Layers_MFC\n",
    "import Caps_basics.CapsNet_Layers as CapsNet_Layers\n",
    "import Caps_basics.ResNetCaps_E as ResNetCaps_E\n",
    "import Fixed_weight_loss\n",
    "import smallNorb as small_norb\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "alpha = 0.0\n",
    "\n",
    "############SET VAR###########\n",
    "CUDA, model_name, db_used = \"cuda:0\", \"CapsNet_MR\", 'smallnorb'\n",
    "log_hm = False\n",
    "FC, CV, USE_CUDA = False, False, True\n",
    "n_epochs, ADAM_LR, ADAM_WD= 100, 0.001, 1e-7\n",
    "lossAN, lossML = True, True\n",
    "D_simplex,U_simplex = True, False\n",
    "onlyML, ML_REC, onlyREC = False,False,True\n",
    "batch_size = 150\n",
    "#######################\n",
    "print(\"ADAM_LR {} ADAM_WD {} USE CUDA {} ON {} model {} dataset {}\".format(ADAM_LR,ADAM_WD,USE_CUDA, CUDA, model_name, db_used))\n",
    "print(\"loss type:AN {} ML {}\".format(lossAN,lossML))\n",
    "print(\"loss type:onlyML {} ML_REC {} onlyREC {}\".format(onlyML, ML_REC, onlyREC))\n",
    "###########################\n",
    "folder_results = \"ALPHA_U_NoRec_Fixed_weight_loss_train_moreFC_\"+db_used\n",
    "implementation_name = str(alpha)+db_used\n",
    "imageFolder = 'missClassifiedImages'\n",
    "#######################\n",
    "if not os.path.exists(folder_results): \n",
    "    os.mkdir(folder_results)\n",
    "if not os.path.exists(os.path.join(folder_results,str(alpha)+\"model_log\")) and os.path.exists(folder_results): \n",
    "    os.mkdir(os.path.join(folder_results,str(alpha)+\"model_log\"))\n",
    "implementation_folder_name = os.path.join(folder_results,implementation_name)\n",
    "\n",
    "with open(implementation_folder_name+\".txt\", \"w\") as text_file:\n",
    "    text_file.write(\"ADAM_LR {} ADAM_WD {} USE CUDA {} ON {} model {} dataset {} loss AN {} ML {}\".format(ADAM_LR,ADAM_WD,USE_CUDA, CUDA, model_name, db_used,lossAN,lossML))\n",
    "    text_file.write(\"loss type:onlyML {} ML_REC {} onlyREC {}\".format(onlyML, ML_REC, onlyREC))\n",
    "with open(implementation_folder_name+\"TEST.txt\", \"w\") as text_file:\n",
    "    text_file.write(\"ALPHA {}, database {}\\n\".format(alpha,db_used))\n",
    "\n",
    "print(\" saved in  \",implementation_name)\n",
    "def lr_decrease(optimizer, lr_decay):  \n",
    "    for param_group in optimizer.param_groups:\n",
    "        init_lr = param_group['lr'] \n",
    "        param_group['lr'] = init_lr*lr_decay\n",
    "        \n",
    "def isnan(x):\n",
    "    return x != x   \n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)    \n",
    "\n",
    "def compute_loss(LossAN,LossML,loss_AN,loss_ML,alpha):\n",
    "    if LossAN and not LossML:\n",
    "        loss = loss_AN\n",
    "    elif LossML and not LossAN:\n",
    "        loss = loss_ML\n",
    "    elif LossML and LossAN:\n",
    "        loss = alpha*loss_AN + (1-alpha)*loss_ML\n",
    "    return loss\n",
    "\n",
    "\n",
    "if(model_name == \"ResNetCaps\"):\n",
    "    resize_dim = (224,224)\n",
    "else:\n",
    "    resize_dim = (32,32)\n",
    "\n",
    "\n",
    "if db_used == 'cifar10':\n",
    "    dataset_transform = transforms.Compose([\n",
    "        transforms.Resize(resize_dim),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "\n",
    "    NUM_CLASSES = 10\n",
    "    print(\"CIFAR10\")\n",
    "    image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform),'val': datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    MNIST_bo = False\n",
    "elif db_used == 'mnist':\n",
    "    dataset_transform = transforms.Compose([    \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    NUM_CLASSES = 10\n",
    "    print(\"MNIST\")\n",
    "    image_datasets = {'train': datasets.MNIST('../data', train=True, download=True, transform=dataset_transform),'val': datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    MNIST_bo = True\n",
    "elif db_used == 'smallnorb':\n",
    "    train_transform = transforms.Compose([\n",
    "                          transforms.Resize(48),\n",
    "                          transforms.RandomCrop(32),\n",
    "                          transforms.ColorJitter(brightness=32./255, contrast=0.5),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.0,), (0.3081,))\n",
    "                        ])\n",
    "    test_transform = transforms.Compose([\n",
    "                          transforms.Resize(48),\n",
    "                          transforms.CenterCrop(32),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.,), (0.3081,))\n",
    "                        ])\n",
    "    NUM_CLASSES = 5\n",
    "    print(\"SMALLNORB\")\n",
    "    path = os.path.join(\"/home/rita/JupyterProjects/EYE-SEA/DataSets\", \"small_norb_root\")\n",
    "    image_datasets = {'train': small_norb.SmallNORB(path, train=True, download=True, transform=train_transform),'val': small_norb.SmallNORB(path, train=False, transform=test_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    smallNORB_bo = True\n",
    "else:\n",
    "    print('Unknown dataset')\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=> using model CapsuleNET with the new loss\")\n",
    "device = torch.device(CUDA if torch.cuda.is_available() else \"cpu\")\n",
    "if model_name == \"ResNetCaps\":\n",
    "    model = ResNetCaps_E.ResNetCaps(NUM_CLASSES)\n",
    "elif model_name == \"CapsNet\":\n",
    "    model = CapsNet_Layers.CapsNet(NUM_CLASSES,FC)\n",
    "else: #CapsNet_MR\n",
    "    model = CapsNet_Layers_MFC.CapsNet_MR(NUM_CLASSES,FC,CV, MNIST = False,smallNORB = smallNORB_bo)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)#cuda()\n",
    "    print('cuda')\n",
    "    \n",
    "optimizer = AdamW.AdamW(model.parameters(),lr = ADAM_LR,  weight_decay=ADAM_WD)\n",
    "scheduler = CyclicLRWithRestarts.CyclicLRWithRestarts(optimizer, batch_size, 60000, restart_period=5, t_mult=1.2, policy=\"cosine\")\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "######NEWLOSS\n",
    "criterionNew = Fixed_weight_loss.Fixed_weight_loss(device,D_simplex,U_simplex,in_feature=NUM_CLASSES,out_feature=NUM_CLASSES)\n",
    "criterionNew = criterionNew.to(device)\n",
    "#############\n",
    "\n",
    "accuracy_train,loss_train,loss_train_AN,loss_train_ML = [],[],[],[]\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs): \n",
    "    model.train() \n",
    "    scheduler.step() #<----------------------------------------------adamwr\n",
    "    print('epoch {}:{}'.format(epoch+1, n_epochs))     \n",
    "    train_loss,train_loss_angle,train_loss_margin,train_accuracy = 0,0,0,0\n",
    "    losted = 0\n",
    "    batch_accuracy = []\n",
    "    if log_hm:\n",
    "        folder_name = \"heatmap/epoch_\"+str(epoch)\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "    \n",
    "    for batch_id, (data, target) in enumerate(dataloaders['train']):\n",
    "        if log_hm and batch_id==0:\n",
    "            fig, ax = plt.subplots()\n",
    "            A = data[1,0,:,:].numpy()\n",
    "            im = ax.imshow(A)\n",
    "            plt.savefig(folder_name+\"/\"+str(batch_id)+\"image\"+str(target[1].item)+str(epoch)+\".jpg\")\n",
    "        \n",
    "        target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "        target_m = []\n",
    "        for i in range(len(target)):\n",
    "            n_loc = (target[i,:] == 1).nonzero()\n",
    "            m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "            m[n_loc,n_loc] = 1\n",
    "            target_m.append(m)\n",
    "        del m \n",
    "        target_m = torch.stack(target_m).to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if model_name == \"ResNetCaps\":\n",
    "            output = model(data)       \n",
    "        else:\n",
    "            output_digit, recostruction,masked, output_fc = model(data)  \n",
    "        if FC or CV: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "        else: output = output_digit    \n",
    "            \n",
    "        #########NEWLOSS########\n",
    "        L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=0)\n",
    "        del target_m\n",
    "#############################################only diagonal#####################################################              \n",
    "        b = []\n",
    "        for i in range(len(L_angle)):\n",
    "            b.append(torch.diag(L_angle[i]))\n",
    "        b = torch.stack(b)\n",
    "        _,label = torch.max(target, 1)\n",
    "\n",
    "        loss_AN =criterion(b,label.long())\n",
    "        \n",
    "################################################################################################################\n",
    "# In CapsNet la loss del modello tiene di conto sia dell'output dell'encoding quanto quello del decoding\n",
    "############################################## marginal loss ###################################################\n",
    "        if lossML:\n",
    "            if onlyML:\n",
    "                loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "            elif ML_REC:\n",
    "                loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "            elif onlyREC:\n",
    "                loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "        else:\n",
    "            loss_ML = loss_AN\n",
    "        loss = compute_loss(lossAN,lossML,loss_AN,loss_ML,alpha)\n",
    "################################################################################################################\n",
    " \n",
    "        if isnan(loss):\n",
    "            print(\"loss lost batch_id\",batch_id)\n",
    "            losted += 1\n",
    "        else:\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print( batch_id)\n",
    "            scheduler.batch_step() #<--------------------------------------------adamwr\n",
    "            train_loss +=float(loss.data)\n",
    "            train_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)))\n",
    "            train_loss_angle += float(loss_AN.data)\n",
    "            train_loss_margin += float(loss_ML.data)\n",
    "\n",
    "            if batch_id % 1000 == 0:\n",
    "                print(\"BATCH_{}: train diag accuracy: {}\".format(batch_id,sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                       np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "                print(\"BATCH_{}: angle loss {} margin loss {}\".format(batch_id,loss_AN.data,loss_ML))\n",
    "\n",
    "                batch_accuracy.append(float(sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                       np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "        del data, target\n",
    "    accuracy_train.append(np.mean(batch_accuracy))\n",
    "    loss_train.append(train_loss/(len(dataloaders['train'])-losted))\n",
    "    loss_train_AN.append(train_loss_angle/(len(dataloaders['train'])-losted))\n",
    "    loss_train_ML.append(train_loss_margin/(len(dataloaders['train'])-losted))\n",
    "\n",
    "    del b, L_angle, loss_AN, loss_ML, output_digit, output_fc, masked, batch_accuracy\n",
    "\n",
    "    if epoch % 20 ==0 and not epoch == 0 :\n",
    "        test_loss,test_accuracy = 0,0\n",
    "\n",
    "        start_test = time.time()\n",
    "\n",
    "        for batch_id, (data, target) in enumerate(dataloaders['val']):\n",
    "    \n",
    "            target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "            data, target = Variable(data), Variable(target) \n",
    "            data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "            target_m = []\n",
    "            for i in range(len(target)):\n",
    "                n_loc = (target[i,:] == 1).nonzero()\n",
    "                m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "                m[n_loc,n_loc] = 1\n",
    "                target_m.append(m)\n",
    "            del m \n",
    "            target_m = torch.stack(target_m).to(device)  \n",
    "            output_digit, _,masked, output_fc = model(data)  \n",
    "            if FC: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "            else: output = output_digit          \n",
    "    #########NEWLOSS########\n",
    "            L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=1)\n",
    "            del target_m\n",
    "            b = []\n",
    "            for i in range(len(L_angle)):\n",
    "                b.append(torch.diag(L_angle[i]))\n",
    "            b = torch.stack(b)\n",
    "\n",
    "            _,label = torch.max(target, 1)\n",
    "            loss_AN =criterion(b,label.long())\n",
    "            if lossML:\n",
    "                if onlyML:\n",
    "                    loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "                elif ML_REC:\n",
    "                    loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "                elif onlyREC:\n",
    "                    loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "            else:\n",
    "                loss_ML = 0.0\n",
    "            loss = compute_loss(lossAN,lossML,loss_AN,loss_ML,alpha)\n",
    "            \n",
    "            test_loss += float(loss.data)\n",
    "    \n",
    "            test_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)))\n",
    "    \n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"BATCH_{}: test accuracy:{}\".format(batch_id,sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "                print(\"BATCH_{}: loss {} margin loss {}\".format(batch_id,loss_AN.data,loss_ML))  \n",
    "            del data, target, b\n",
    "        end_test= time.time()   \n",
    "        print(\"Validation time execution {}\".format(end_test - len(dataloaders['val'])))\n",
    "        print(\"Loss value for test phase: {}\".format(test_loss / len(dataloaders['val'])))\n",
    "        print(\"Accuracy value for test phase: {}\".format(test_accuracy / len(dataloaders['val'])))\n",
    "        with open(implementation_folder_name+\"TEST.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"Validation time execution {}\\n\".format(end_test-start_test))\n",
    "            text_file.write(\"Loss value for test phase: {}\\n\".format(test_loss / len(dataloaders['val'])))\n",
    "            text_file.write(\"Accuracy value for test phase: {}\\n\".format(test_accuracy / len(dataloaders['val'])))\n",
    "        del test_loss, test_accuracy, loss, loss_AN, loss_ML, output_digit, output_fc, masked, output\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss_type': implementation_name,\n",
    "            'arch': 'CapsNet',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, folder_results+\"/\"+str(alpha)+\"model_log/checkpoint_\"+\"_\"+model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training time execution {}\".format(end-start))\n",
    "print(\"Loss value for training phase: {}\".format(train_loss / len(dataloaders['train'])))\n",
    "print(\"Accuracy value for training phase: {}\".format(train_accuracy / len(dataloaders['train'])))\n",
    "\n",
    "with open(implementation_folder_name+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"Training time execution {}\\n\".format(end-start))\n",
    "    text_file.write(\"Loss value for training phase: {}\\n\".format(train_loss / len(dataloaders['train'])))\n",
    "    text_file.write(\"Accuracy value for training phase: {}\\n\".format(train_accuracy / len(dataloaders['train'])))\n",
    "save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'loss_type': implementation_name,\n",
    "        'arch': 'CapsNet',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    },folder_results+\"/\"+str(alpha)+\"model_log/checkpoint_\"+\"_\"+model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "\n",
    "    \n",
    "epochs = np.arange(1,n_epochs+1)\n",
    "plt.plot(epochs, loss_train, color='g')\n",
    "plt.plot(epochs, loss_train_AN, color='b')\n",
    "plt.plot(epochs, loss_train_ML, color='c')\n",
    "plt.plot(epochs, accuracy_train, color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy - Loss')\n",
    "plt.title('Training phase')\n",
    "plt.savefig(folder_results+\"/\"+implementation_name+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss,test_accuracy = 0,0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for batch_id, (data, target) in enumerate(dataloaders['val']):\n",
    "    \n",
    "    target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "    target_m = []\n",
    "    for i in range(len(target)):\n",
    "        n_loc = (target[i,:] == 1).nonzero()\n",
    "        m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "        m[n_loc,n_loc] = 1\n",
    "        target_m.append(m) \n",
    "    target_m = torch.stack(target_m).to(device)  \n",
    "    output_digit, _,masked, output_fc = model(data)  \n",
    "    if FC or CV: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "    else: output = output_digit          \n",
    "    #########NEWLOSS########\n",
    "    L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=1)\n",
    "    \n",
    "    b = []\n",
    "    for i in range(len(L_angle)):\n",
    "        b.append(torch.diag(L_angle[i]))\n",
    "    b = torch.stack(b)\n",
    "\n",
    "    _,label = torch.max(target, 1)\n",
    "    loss_AN =criterion(b,label.long())\n",
    "    if lossML:\n",
    "        if onlyML:\n",
    "            loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "        elif ML_REC:\n",
    "            loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "        elif onlyREC:\n",
    "            loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "    else:\n",
    "        loss_ML = 0.0\n",
    "    loss = compute_loss(lossAN,lossML,loss_AN,loss_ML,alpha)\n",
    "    test_loss += float(loss.data)\n",
    "    test_accuracy += float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))) / float(data.size(0))\n",
    "    \n",
    "    if batch_id % 100 == 0:\n",
    "            print(\"BATCH_{}: test accuracy:\".format(batch_id, sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)) ))    \n",
    "            print(\"BATCH_{}:loss {} margin loss {}\" .format(batch_id,loss.data,loss_ML))\n",
    "end = time.time()   \n",
    "print(\"Validation time execution {}\".format(end-start))\n",
    "print(\"Loss value for test phase: {}\".format(test_loss / len(dataloaders['val'])))\n",
    "print(\"Accuracy value for test phase: {}\".format(test_accuracy / len(dataloaders['val'])))\n",
    "with open(implementation_folder_name+\"TEST.txt\", \"a\") as text_file:\n",
    "    text_file.write(\"Validation time execution {}\\n\".format(end-start))\n",
    "    text_file.write(\"Loss value for test phase: {}\\n\".format(test_loss / len(dataloaders['val'])))\n",
    "    text_file.write(\"Accuracy value for test phase: {}\\n\".format(test_accuracy / len(dataloaders['val'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
