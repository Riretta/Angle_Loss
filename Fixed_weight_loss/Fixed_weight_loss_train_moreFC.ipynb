{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import import_ipynb\n",
    "import adamw.cyclic_scheduler as CyclicLRWithRestarts\n",
    "import adamw.adamw as AdamW\n",
    "import Caps_basics.CapsNet_Layers_multiFC as CapsNet_Layers_MFC\n",
    "import Caps_basics.CapsNet_Layers as CapsNet_Layers\n",
    "import Caps_basics.ResNetCaps_E as ResNetCaps_E\n",
    "import Fixed_weight_loss\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "############SET VAR###########\n",
    "implementation_name = \"Fixed_weight_loss_train_moreFC\"\n",
    "CUDA, model_name, db_used = \"cuda:0\", \"CapsNet_MR\", 'mnist'\n",
    "log_hm = False\n",
    "FC, CV, USE_CUDA = False, False, True\n",
    "n_epochs, ADAM_LR, ADAM_WD= 100, 0.01, 1e-7\n",
    "batch_size = 150\n",
    "#######################\n",
    "print(\"ADAM_LR {} ADAM_WD {} USE CUDA {} ON {} model {} dataset {}\".format(ADAM_LR,ADAM_WD,USE_CUDA, CUDA, model_name, db_used))\n",
    "\n",
    "def lr_decrease(optimizer, lr_clip):  \n",
    "    for param_group in optimizer.param_groups:\n",
    "        init_lr = param_group['lr'] \n",
    "        param_group['lr'] = init_lr*lr_clip\n",
    "        \n",
    "def isnan(x):\n",
    "    return x != x   \n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)    \n",
    "\n",
    "if(model_name == \"ResNetCaps\"):\n",
    "    resize_dim = (224,224)\n",
    "else:\n",
    "    resize_dim = (32,32)\n",
    "\n",
    "\n",
    "if db_used == 'cifar10':\n",
    "    dataset_transform = transforms.Compose([\n",
    "        transforms.Resize(resize_dim),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "\n",
    "    NUM_CLASSES = 10\n",
    "    print(\"CIFAR10\")\n",
    "    image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform),'val': datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True) }\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    MNIST_bo = False\n",
    "elif db_used == 'mnist':\n",
    "    dataset_transform = transforms.Compose([    \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    NUM_CLASSES = 10\n",
    "    print(\"MNIST\")\n",
    "    image_datasets = {'train': datasets.MNIST('../data', train=True, download=True, transform=dataset_transform),'val': datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=True)}\n",
    "    print(\"Initializing Datasets and Dataloaders...\")\n",
    "    MNIST_bo = True\n",
    "else:\n",
    "    print('Unknown dataset')\n",
    "\n",
    "\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"=> using model CapsuleNET with the new loss\")\n",
    "device = torch.device(CUDA if torch.cuda.is_available() else \"cpu\")\n",
    "if model_name == \"ResNetCaps\":\n",
    "    model = ResNetCaps_E.ResNetCaps(NUM_CLASSES)\n",
    "elif model_name == \"CapsNet\":\n",
    "    model = CapsNet_Layers.CapsNet(NUM_CLASSES,FC)\n",
    "else: #CapsNet_MR\n",
    "    model = CapsNet_Layers_MFC.CapsNet_MR(NUM_CLASSES,FC,CV, MNIST = MNIST_bo)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)#cuda()\n",
    "    print('cuda')\n",
    "    \n",
    "optimizer = AdamW.AdamW(model.parameters(),lr = ADAM_LR,  weight_decay=ADAM_WD)\n",
    "scheduler = CyclicLRWithRestarts.CyclicLRWithRestarts(optimizer, batch_size, 60000, restart_period=5, t_mult=1.2, policy=\"cosine\")\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "######NEWLOSS\n",
    "criterionNew = Fixed_weight_loss.Fixed_weight_loss(device)\n",
    "criterionNew = criterionNew.to(device)\n",
    "#############\n",
    "\n",
    "accuracy_train,loss_train,loss_train_AN,loss_train_ML = [],[],[],[]\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs): \n",
    "    model.train() \n",
    "    scheduler.step() #<----------------------------------------------adamwr\n",
    "    print('epoch {}:{}'.format(epoch+1, n_epochs))     \n",
    "    train_loss,train_loss_angle,train_loss_margin,train_accuracy = 0,0,0,0\n",
    "    \n",
    "    batch_accuracy = []\n",
    "    if log_hm:\n",
    "        folder_name = \"heatmap/epoch_\"+str(epoch)\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "    \n",
    "    for batch_id, (data, target) in enumerate(dataloaders['train']):\n",
    "        if log_hm and batch_id==0:\n",
    "            fig, ax = plt.subplots()\n",
    "            A = data[1,0,:,:].numpy()\n",
    "            im = ax.imshow(A)\n",
    "            plt.savefig(folder_name+\"/\"+str(batch_id)+\"image\"+str(target[1].item)+str(epoch)+\".jpg\")\n",
    "        \n",
    "        target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "        target_m = []\n",
    "        for i in range(len(target)):\n",
    "            n_loc = (target[i,:] == 1).nonzero()\n",
    "            m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "            m[n_loc,n_loc] = 1\n",
    "            target_m.append(m)\n",
    "        del m \n",
    "        target_m = torch.stack(target_m).to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if model_name == \"ResNetCaps\":\n",
    "            output = model(data)       \n",
    "        else:\n",
    "            output_digit, recostruction,masked, output_fc = model(data)  \n",
    "        if FC or CV: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "        else: output = output_digit    \n",
    "            \n",
    "        #########NEWLOSS########\n",
    "        L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=0)\n",
    "        del target_m\n",
    "#############################################only diagonal#####################################################              \n",
    "        b = []\n",
    "        for i in range(len(L_angle)):\n",
    "            b.append(torch.diag(L_angle[i]))\n",
    "        b = torch.stack(b)\n",
    "        _,label = torch.max(target, 1)\n",
    "\n",
    "        loss_AN =criterion(b,label.long())\n",
    "################################################################################################################\n",
    "# In CapsNet la loss del modello tiene di conto sia dell'output dell'encoding quanto quello del decoding\n",
    "############################################## marginal loss ###################################################\n",
    "        loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "        loss = loss_AN + loss_ML\n",
    "################################################################################################################\n",
    " \n",
    "        if isnan(loss):\n",
    "            print(\"loss lost\")\n",
    "            break\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print( batch_id)\n",
    "        scheduler.batch_step() #<--------------------------------------------adamwr\n",
    "        train_loss +=float(loss.data)\n",
    "        train_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "        train_loss_angle += float(loss_AN.data)\n",
    "        train_loss_margin += float(loss_ML.data)\n",
    "\n",
    "        if batch_id % 1000 == 0:\n",
    "            print(\"train diag accuracy:\", sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "            print(\"angle loss {} margin loss {}\" .format(loss_AN.data,loss_ML))\n",
    "\n",
    "            batch_accuracy.append(float(sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)))\n",
    "        del data, target\n",
    "    accuracy_train.append(np.mean(batch_accuracy))\n",
    "    loss_train.append(train_loss/len(dataloaders['train']))\n",
    "    loss_train_AN.append(train_loss_angle/len(dataloaders['train']))\n",
    "    loss_train_ML.append(train_loss_margin/len(dataloaders['train']))\n",
    "\n",
    "    del b, L_angle, loss_AN, loss_ML, output_digit, output_fc, masked, batch_accuracy\n",
    "\n",
    "    if epoch % 10 ==0 and not epoch == 0 :\n",
    "        test_loss,test_accuracy = 0,0\n",
    "\n",
    "        start_test = time.time()\n",
    "\n",
    "        for batch_id, (data, target) in enumerate(dataloaders['val']):\n",
    "    \n",
    "            target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "            data, target = Variable(data), Variable(target) \n",
    "            data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "            target_m = []\n",
    "            for i in range(len(target)):\n",
    "                n_loc = (target[i,:] == 1).nonzero()\n",
    "                m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "                m[n_loc,n_loc] = 1\n",
    "                target_m.append(m)\n",
    "            del m \n",
    "            target_m = torch.stack(target_m).to(device)  \n",
    "            output_digit, _,masked, output_fc = model(data)  \n",
    "            if FC: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "            else: output = output_digit          \n",
    "    #########NEWLOSS########\n",
    "            L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=1)\n",
    "            del target_m\n",
    "            b = []\n",
    "            for i in range(len(L_angle)):\n",
    "                b.append(torch.diag(L_angle[i]))\n",
    "            b = torch.stack(b)\n",
    "\n",
    "            _,label = torch.max(target, 1)\n",
    "            loss_AN =criterion(b,label.long())\n",
    "            loss_ML = criterionNew.margin_loss(output_digit,target) \n",
    "            loss = loss_AN + loss_ML\n",
    "            test_loss += float(loss.data)\n",
    "    \n",
    "            test_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    \n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"test accuracy:\", sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size) )    \n",
    "                print(\"loss {} margin loss {}\" .format(loss_AN.data,loss_ML))  \n",
    "            del data, target, b\n",
    "        end_test= time.time()   \n",
    "        print(\"Validation time execution {}\".format(end_test - start_test))\n",
    "        print(\"Loss value for test phase: {}\".format(test_loss / batch_id)) #len(dataloaders['val'])))\n",
    "        print(\"Accuracy value for test phase: {}\".format(test_accuracy / batch_id)) #len(dataloaders['val'])))\n",
    "        with open(implementation_name+\"TEST.txt\", \"a\") as text_file:\n",
    "            text_file.write(\"Validation time execution {}\\\\\".format(end_test-start_test))\n",
    "            text_file.write(\"Loss value for test phase: {}\\\\\".format(test_loss / batch_id)) #len(dataloaders['val'])))\n",
    "            text_file.write(\"Accuracy value for test phase: {}\\\\\".format(test_accuracy / batch_id )) # len(dataloaders['val'])))\n",
    "        del test_loss, test_accuracy, loss, loss_AN, loss_ML, output_digit, output_fc, masked, output\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'loss_type': implementation_name,\n",
    "            'arch': 'CapsNet',\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, \"model_log/checkpoint_\"+implementation_name+\"_\"+model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training time execution {}\".format(end-start))\n",
    "print(\"Loss value for training phase: {}\".format(train_loss / batch_id)) # len(dataloaders['train'])))\n",
    "print(\"Accuracy value for training phase: {}\".format(train_accuracy /  batch_id)) #len(dataloaders['train'])))\n",
    "\n",
    "with open(implementation_name+\".txt\", \"w\") as text_file:\n",
    "    text_file.write(\"Training time execution {}\\\\\".format(end-start))\n",
    "    text_file.write(\"Loss value for training phase: {}\\\\\".format(train_loss / batch_id)) #  len(dataloaders['train'])))\n",
    "    text_file.write(\"Accuracy value for training phase: {}\\\\\".format(train_accuracy /  batch_id)) #len(dataloaders['train'])))\n",
    "save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'loss_type': implementation_name,\n",
    "        'arch': 'CapsNet',\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, \"checkpoint_\"+implementation_name+\"_\"+model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "    \n",
    "epochs = np.arange(1,n_epochs+1)\n",
    "plt.plot(epochs, loss_train, color='g')\n",
    "plt.plot(epochs, loss_train_AN, color='b')\n",
    "plt.plot(epochs, loss_train_ML, color='c')\n",
    "plt.plot(epochs, accuracy_train, color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy - Loss')\n",
    "plt.title('Training phase')\n",
    "plt.savefig(implementation_name+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "test_loss,test_accuracy = 0,0\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for batch_id, (data, target) in enumerate(dataloaders['val']):\n",
    "    \n",
    "    target =torch.eye(NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "    target_m = []\n",
    "    for i in range(len(target)):\n",
    "        n_loc = (target[i,:] == 1).nonzero()\n",
    "        m = torch.zeros(NUM_CLASSES,NUM_CLASSES)\n",
    "        m[n_loc,n_loc] = 1\n",
    "        target_m.append(m) \n",
    "    target_m = torch.stack(target_m).to(device)  \n",
    "    output_digit, _,masked, output_fc = model(data)  \n",
    "    if FC or CV: output = output_fc.view(output_fc.size(0),NUM_CLASSES,-1)\n",
    "    else: output = output_digit          \n",
    "    #########NEWLOSS########\n",
    "    L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=1)\n",
    "    \n",
    "    b = []\n",
    "    for i in range(len(L_angle)):\n",
    "        b.append(torch.diag(L_angle[i]))\n",
    "    b = torch.stack(b)\n",
    "\n",
    "    _,label = torch.max(target, 1)\n",
    "    loss_AN =criterion(b,label.long())\n",
    "    loss_ML = criterionNew.margin_loss(output_digit,target) \n",
    "    loss = loss_AN + loss_ML\n",
    "    test_loss += float(loss.data)\n",
    "    test_accuracy += float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size))\n",
    "    \n",
    "    if batch_id % 100 == 0:\n",
    "            print(\"test accuracy:\", sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                   np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size) )    \n",
    "            print(\"loss {} margin loss {}\" .format(loss.data,loss_ML))\n",
    "end = time.time()   \n",
    "print(\"Validation time execution {}\".format(end-start))\n",
    "print(\"Loss value for test phase: {}\".format(test_loss / batch_id)) #len(dataloaders['val'])))\n",
    "print(\"Accuracy value for test phase: {}\".format(test_accuracy / batch_id)) #len(dataloaders['val'])))\n",
    "with open(implementation_name+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"Validation time execution {}\\\\\".format(end-start))\n",
    "    text_file.write(\"Loss value for test phase: {}\\\\\".format(test_loss / batch_id)) #len(dataloaders['val'])))\n",
    "    text_file.write(\"Accuracy value for test phase: {}\\\\\".format(test_accuracy / batch_id )) # len(dataloaders['val'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
