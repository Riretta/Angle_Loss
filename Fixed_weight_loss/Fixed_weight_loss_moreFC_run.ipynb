{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import csv\n",
    "from skimage import io\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import import_ipynb\n",
    "import adamw.cyclic_scheduler as CyclicLRWithRestarts\n",
    "import adamw.adamw as AdamW\n",
    "import Caps_basics.CapsNet_Layers_multiFC as CapsNet_Layers_MFC\n",
    "import Caps_basics.CapsNet_Layers as CapsNet_Layers\n",
    "import Caps_basics.ResNetCaps_E as ResNetCaps_E\n",
    "import Fixed_weight_loss\n",
    "import smallNorb as small_norb\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class Fixed_weight_loss_MFC(nn.Module):\n",
    "\n",
    "    def __init__(self, dataset=\"mnist\", batch_size = 128, n_epochs = 100, bool_alpha= True, alpha = 0.0, save_files = False):\n",
    "        super(Fixed_weight_loss_MFC, self).__init__()\n",
    "        ############SET VAR###########\n",
    "        self.CUDA, self.USE_CUDA = \"cuda:0\",True #<-------------------------cuda\n",
    "        \n",
    "        self.model_name, self.FC, self.CV, self.db_used =  \"CapsNet_MR\", False, False, dataset  #<----model\n",
    "          \n",
    "        self.n_epochs, self.batch_size, self.ADAM_LR, self.ADAM_WD = n_epochs, batch_size, 0.001, 1e-7 #<---optimizer\n",
    "        \n",
    "        self.lossAN, self.lossML = True, True #<-----------------------loss required\n",
    "        self.onlyML, self.ML_REC, self.onlyREC = True,False,False #<-----------------loss computed\n",
    "        self.D_simplex,self.U_simplex = False, True #<----------simplex distribution\n",
    "\n",
    "        self.alpha, self.bool_alpha = alpha, bool_alpha  #<-------------------------alpha\n",
    "        \n",
    "#########################################LOG if required########################        \n",
    "        self.save_files = save_files\n",
    "        if self.save_files:\n",
    "            print(\" saved in {}: train {}, test {} \".format(self.folder_results,self.implementation_folder_name+\".txt\",self.implementation_folder_name+\"TEST.txt\"))\n",
    "            self.folder_results = \"ALPHA_U_NoRec_Fixed_weight_loss_train_moreFC_\"+self.db_used\n",
    "            self.implementation_name = str(self.alpha)+self.db_used\n",
    "            if not os.path.exists(self.folder_results): \n",
    "                os.mkdir(self.folder_results)\n",
    "            if not os.path.exists(os.path.join(self.folder_results,str(alpha)+\"model_log\")):\n",
    "                os.mkdir(os.path.join(self.folder_results,str(alpha)+\"model_log\"))\n",
    "            self.implementation_folder_name = os.path.join(self.folder_results,self.implementation_name)\n",
    "            with open(self.implementation_folder_name+\".txt\", \"w\") as text_file:\n",
    "                text_file.write(\"self.ADAM_LR {} self.ADAM_WD {} USE CUDA {} ON {} model {} dataset {} loss AN {} ML {}\\n\".format(self.ADAM_LR,self.ADAM_WD,self.USE_CUDA, self.CUDA, self.model_name, self.db_used, self.lossAN,self.lossML))\n",
    "                text_file.write(\"loss type:self.onlyML {} self.ML_REC {} self.onlyREC {}\\n\".format(self.onlyML, self.ML_REC, self.onlyREC))\n",
    "                text_file.write(\"self.D_simplex {} self.U_simplex {}\\n\".format(self.D_simplex,self.U_simplex))\n",
    "            \n",
    "            with open(self.implementation_folder_name+\"TEST.txt\", \"w\") as text_file:\n",
    "                text_file.write(\"ALPHA {}, database {}\\n\".format(self.alpha,self.db_used))\n",
    "################################################################  \n",
    "        print(\"self.ADAM_LR {} self.ADAM_WD {} USE CUDA {} ON {} model {}; \\n self.D_simplex {} self.U_simplex {}\\n\".format(self.ADAM_LR,self.ADAM_WD,self.USE_CUDA, self.CUDA, self.model_name, self.D_simplex,self.U_simplex))\n",
    "        print(\"Loss Type: \",self.lossAN,self.lossML) \n",
    "        print(\"loss type:self.onlyML {} self.ML_REC {} self.onlyREC {}\".format(self.onlyML, self.ML_REC, self.onlyREC))\n",
    "\n",
    "################################setting dataset#####################################\n",
    "        resize_dim = (32,32)\n",
    "        self.MNIST_bo, self.smallNORB_bo = False, False\n",
    "        if self.db_used == 'cifar10':\n",
    "            dataset_transform = transforms.Compose([\n",
    "                transforms.Resize(resize_dim),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "            ])\n",
    "\n",
    "            self. NUM_CLASSES = 10\n",
    "            print(\"CIFAR10\")\n",
    "            self.image_datasets = {'train': datasets.CIFAR10('../data', train=True, download=True, transform=dataset_transform), 'val': datasets.CIFAR10('../data', train=False, download=True, transform=dataset_transform)}\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "            self.dataloaders = {'train': torch.utils.data.DataLoader(self.image_datasets['train'], batch_size=self.batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(self.image_datasets['val'], batch_size=self.batch_size, shuffle=True) }\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "        elif self.db_used == 'mnist':\n",
    "            dataset_transform = transforms.Compose([    \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "            self.NUM_CLASSES = 10\n",
    "            print(\"MNIST\")\n",
    "            self.image_datasets = {'train': datasets.MNIST('../data', train=True, download=True, transform=dataset_transform),'val': datasets.MNIST('../data', train=False, download=True, transform=dataset_transform)}\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "            self.dataloaders = {'train': torch.utils.data.DataLoader(self.image_datasets['train'], batch_size=self.batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(self.image_datasets['val'], batch_size=self.batch_size, shuffle=True)}\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "            self.MNIST_bo = True\n",
    "        elif self.db_used == 'smallnorb':\n",
    "            train_transform = transforms.Compose([\n",
    "                                  transforms.Resize(48),\n",
    "                                  transforms.RandomCrop(32),\n",
    "                                  transforms.ColorJitter(brightness=32./255, contrast=0.5),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.0,), (0.3081,))\n",
    "                                ])\n",
    "            test_transform = transforms.Compose([\n",
    "                                  transforms.Resize(48),\n",
    "                                  transforms.CenterCrop(32),\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.,), (0.3081,))\n",
    "                                ])\n",
    "            self.NUM_CLASSES = 5\n",
    "            print(\"SMALLNORB\")\n",
    "            path = os.path.join(\"/home/rita/JupyterProjects/EYE-SEA/DataSets\", \"small_norb_root\")\n",
    "            self.image_datasets = {'train': small_norb.SmallNORB(path, train=True, download=True, transform=train_transform),'val': small_norb.SmallNORB(path, train=False, transform=test_transform)}\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "            self.dataloaders = {'train': torch.utils.data.DataLoader(self.image_datasets['train'], batch_size=self.batch_size, shuffle=True) , 'val': torch.utils.data.DataLoader(self.image_datasets['val'], batch_size=self.batch_size, shuffle=True)}\n",
    "            print(\"Initializing Datasets and Dataloaders...\")\n",
    "            self.smallNORB_bo = True\n",
    "        else:\n",
    "            print('Unknown dataset')\n",
    "\n",
    "        self.dataset_sizes = {x: len(self.image_datasets[x]) for x in ['train', 'val']}\n",
    "        \n",
    "        \n",
    "        \n",
    "    def lr_decrease(self,optimizer, lr_decay):  \n",
    "        for param_group in optimizer.param_groups:\n",
    "            init_lr = param_group['lr'] \n",
    "            param_group['lr'] = init_lr*lr_decay\n",
    "\n",
    "    def isnan(self,x):\n",
    "        return x != x   \n",
    "\n",
    "    def save_checkpoint(self,state, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)    \n",
    "\n",
    "    def compute_loss(self,loss_AN,loss_ML):\n",
    "        if self.lossAN and not self.lossML:\n",
    "            loss = loss_AN\n",
    "        elif self.lossAN and not self.lossML:\n",
    "            loss = loss_ML\n",
    "        elif self.lossML and self.lossAN and not self.bool_alpha:\n",
    "            loss = loss_AN + loss_ML\n",
    "        elif self.lossML and self.lossAN and self.bool_alpha:\n",
    "            loss = self.alpha*loss_AN + (1-self.alpha)*loss_ML\n",
    "        return loss\n",
    "\n",
    "# In[ ]:\n",
    "    def procedure_train(self):\n",
    "        print(\"=> using model CapsuleNET with the new loss\")\n",
    "        device = torch.device(self.CUDA if torch.cuda.is_available() else \"cpu\")\n",
    "        model = CapsNet_Layers_MFC.CapsNet_MR(self.NUM_CLASSES,self.FC,self.CV, MNIST = False,smallNORB = self.smallNORB_bo)\n",
    "\n",
    "        if self.USE_CUDA:\n",
    "            model = model.to(device)\n",
    "            print('load model on cuda')\n",
    "\n",
    "        optimizer = AdamW.AdamW(model.parameters(),lr = self.ADAM_LR,  weight_decay=self.ADAM_WD)\n",
    "        scheduler = CyclicLRWithRestarts.CyclicLRWithRestarts(optimizer, self.batch_size, len(self.image_datasets['train']), restart_period=5, t_mult=1.2, policy=\"cosine\")\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        ######NEWLOSS\n",
    "        criterionNew = Fixed_weight_loss.Fixed_weight_loss(device, self.D_simplex, self.U_simplex, in_feature=self.NUM_CLASSES, out_feature=self.NUM_CLASSES )\n",
    "        criterionNew = criterionNew.to(device)\n",
    "        #############\n",
    "\n",
    "        accuracy_train,loss_train,loss_train_AN,loss_train_ML = [],[],[],[]\n",
    "\n",
    "        start = time.time()\n",
    "        for epoch in range(self.n_epochs): \n",
    "            model.train() \n",
    "            scheduler.step() #<----------------------------------------------adamwr\n",
    "            print('TRAINING: epoch {}:{}'.format(epoch+1, self.n_epochs))     \n",
    "            train_loss,train_loss_angle,train_loss_margin,train_accuracy = 0,0,0,0\n",
    "            losted = 0\n",
    "            batch_accuracy = []\n",
    "           \n",
    "            for batch_id, (data, target) in enumerate(self.dataloaders['train']):\n",
    "                if self.log_hm and batch_id==0:\n",
    "                    fig, ax = plt.subplots()\n",
    "                    A = data[1,0,:,:].numpy()\n",
    "                    im = ax.imshow(A)\n",
    "                    plt.savefig(folder_name+\"/\"+str(batch_id)+\"image\"+str(target[1].item)+str(epoch)+\".jpg\")\n",
    "\n",
    "                target =torch.eye(self.NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "                data, target = Variable(data), Variable(target)\n",
    "                data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "                target_m = []\n",
    "                for i in range(len(target)):\n",
    "                    n_loc = (target[i,:] == 1).nonzero()\n",
    "                    m = torch.zeros(self.NUM_CLASSES,self.NUM_CLASSES)\n",
    "                    m[n_loc,n_loc] = 1\n",
    "                    target_m.append(m)\n",
    "                del m \n",
    "                target_m = torch.stack(target_m).to(device)  \n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                output_digit, recostruction,masked, output_fc = model(data)  \n",
    "                if self.FC or self.CV: output = output_fc.view(output_fc.size(0),self.NUM_CLASSES,-1)\n",
    "                else: output = output_digit    \n",
    "\n",
    "                #########NEWLOSS########\n",
    "                L_angle = criterionNew.arc_loss(output.squeeze(),target_m,val=0)\n",
    "                del target_m\n",
    "        #############################################only diagonal#####################################################              \n",
    "                b = []\n",
    "                for i in range(len(L_angle)):\n",
    "                    b.append(torch.diag(L_angle[i]))\n",
    "                b = torch.stack(b)\n",
    "                _,label = torch.max(target, 1)\n",
    "\n",
    "                loss_AN =criterion(b,label.long())\n",
    "\n",
    "        ################################################################################################################\n",
    "        ############################################### marginal loss ##################################################\n",
    "                if self.lossML:\n",
    "                    if self.onlyML:\n",
    "                        loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "                    elif self.ML_REC:\n",
    "                        loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "                    elif self.onlyREC:\n",
    "                        loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "                else:\n",
    "                    loss_ML = loss_AN\n",
    "                loss = self.compute_loss(loss_AN,loss_ML)\n",
    "        ################################################################################################################\n",
    "\n",
    "                if self.isnan(loss):\n",
    "                    print(\"loss lost batch_id\",batch_id)\n",
    "                    losted += 1\n",
    "                else:\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    #print( batch_id)\n",
    "                    scheduler.batch_step() #<--------------------------------------------adamwr\n",
    "                    train_loss +=float(loss.data)\n",
    "                    train_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)))\n",
    "                    train_loss_angle += float(loss_AN.data)\n",
    "                    train_loss_margin += float(loss_ML.data)\n",
    "\n",
    "                    if batch_id % 1000 == 0:\n",
    "                        print(\"BATCH_{}: train diag accuracy: {}\".format(batch_id,sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                               np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "                        print(\"BATCH_{}: angle loss {} margin loss {}\".format(batch_id,loss_AN.data,loss_ML))\n",
    "\n",
    "                        batch_accuracy.append(float(sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                               np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "                del data, target\n",
    "            accuracy_train.append(np.mean(batch_accuracy))\n",
    "            loss_train.append(train_loss/(len(self.dataloaders['train'])-losted))\n",
    "            loss_train_AN.append(train_loss_angle/(len(self.dataloaders['train'])-losted))\n",
    "            loss_train_ML.append(train_loss_margin/(len(self.dataloaders['train'])-losted))\n",
    "\n",
    "            del b, L_angle, loss_AN, loss_ML, output_digit, output_fc, masked, batch_accuracy\n",
    "\n",
    "            if epoch % 20 ==0 and not epoch == 0 :\n",
    "                test_loss,test_accuracy = 0,0\n",
    "\n",
    "                start_test = time.time()\n",
    "\n",
    "                for batch_id, (data, target) in enumerate(self.dataloaders['val']):\n",
    "\n",
    "                    target =torch.eye(self.NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "                    data, target = Variable(data), Variable(target) \n",
    "                    data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "                    target_m = []\n",
    "                    for i in range(len(target)):\n",
    "                        n_loc = (target[i,:] == 1).nonzero()\n",
    "                        m = torch.zeros(self.NUM_CLASSES,self.NUM_CLASSES)\n",
    "                        m[n_loc,n_loc] = 1\n",
    "                        target_m.append(m)\n",
    "                    del m \n",
    "                    target_m = torch.stack(target_m).to(device)  \n",
    "                    output_digit, _,masked, output_fc = model(data)  \n",
    "                    if self.FC: output = output_fc.view(output_fc.size(0),self.NUM_CLASSES,-1)\n",
    "                    else: output = output_digit          \n",
    "            #########NEWLOSS########\n",
    "                    L_angle = criterionNew.arc_loss(output.squeeze(),target_m,epoch,batch_id,\"heatmap/\",val=1)\n",
    "                    del target_m\n",
    "                    b = []\n",
    "                    for i in range(len(L_angle)):\n",
    "                        b.append(torch.diag(L_angle[i]))\n",
    "                    b = torch.stack(b)\n",
    "\n",
    "                    _,label = torch.max(target, 1)\n",
    "                    loss_AN =criterion(b,label.long())\n",
    "                    if self.lossML:\n",
    "                        if self.onlyML:\n",
    "                            loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "                        elif self.ML_REC:\n",
    "                            loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "                        elif self.onlyREC:\n",
    "                            loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "                    else:\n",
    "                        loss_ML = 0.0\n",
    "                    loss = self.compute_loss(loss_AN,loss_ML)\n",
    "\n",
    "                    test_loss += float(loss.data)\n",
    "                    test_accuracy +=float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)))\n",
    "\n",
    "                    if batch_id % 100 == 0:\n",
    "                        print(\"BATCH_{}: test accuracy:{}\".format(batch_id,sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                           np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0))))\n",
    "                        print(\"BATCH_{}: loss {} margin loss {}\".format(batch_id,loss_AN.data,loss_ML))  \n",
    "                    del data, target, b\n",
    "                end_test= time.time()  \n",
    "                \n",
    "                print(\"VALIDATION{}:Validation time execution {}\".format(epoch,end_test - len(self.dataloaders['val'])))\n",
    "                print(\"VALIDATION{}:Loss value for test phase: {}\".format(epoch,test_loss / len(self.dataloaders['val'])))\n",
    "                print(\"VALIDATION{}:Accuracy value for test phase: {}\".format(epoch,test_accuracy / len(self.dataloaders['val'])))\n",
    "                if \n",
    "                with open(self.implementation_folder_name+\"TEST.txt\", \"a\") as text_file:\n",
    "                    text_file.write(\"Validation time execution {}\\n\".format(end_test-start_test))\n",
    "                    text_file.write(\"Loss value for test phase: {}\\n\".format(test_loss / len(self.dataloaders['val'])))\n",
    "                    text_file.write(\"Accuracy value for test phase: {}\\n\".format(test_accuracy / len(self.dataloaders['val'])))\n",
    "                del test_loss, test_accuracy, loss, loss_AN, loss_ML, output_digit, output_fc, masked, output\n",
    "                self.save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'loss_type': self.implementation_name,\n",
    "                    'arch': 'CapsNet',\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, self.folder_results+\"/\"+str(self.alpha)+\"model_log/checkpoint_\"+\"_\"+self.model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"TRAIN: Training time execution {}\".format(end-start))\n",
    "        print(\"TRAIN: Loss value for training phase: {}\".format(train_loss / len(self.dataloaders['train'])))\n",
    "        print(\"TRAIN: Accuracy value for training phase: {}\".format(train_accuracy / len(self.dataloaders['train'])))\n",
    "        if self.save_files:\n",
    "            with open(self.implementation_folder_name+\".txt\", \"a\") as text_file:\n",
    "                text_file.write(\"Training time execution {}\\n\".format(end-start))\n",
    "                text_file.write(\"Loss value for training phase: {}\\n\".format(train_loss / len(self.dataloaders['train'])))\n",
    "                text_file.write(\"Accuracy value for training phase: {}\\n\".format(train_accuracy / len(self.dataloaders['train'])))\n",
    "            self.save_checkpoint({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'loss_type': self.implementation_name,\n",
    "                    'arch': 'CapsNet',\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                },self.folder_results+\"/\"+str(self.alpha)+\"model_log/checkpoint_\"+\"_\"+self.model_name+\"_\"+str(epoch)+\".pth.tar\")\n",
    "\n",
    "\n",
    "            epochs = np.arange(1,self.n_epochs+1)\n",
    "            plt.plot(epochs, loss_train, color='g')\n",
    "            plt.plot(epochs, loss_train_AN, color='b')\n",
    "            plt.plot(epochs, loss_train_ML, color='c')\n",
    "            plt.plot(epochs, accuracy_train, color='orange')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy - Loss')\n",
    "            plt.title('Training phase')\n",
    "            plt.savefig(self.folder_results+\"/\"+self.implementation_name+\".png\")\n",
    "\n",
    "        model.eval()\n",
    "        test_loss,test_accuracy = 0,0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_id, (data, target) in enumerate(self.dataloaders['val']):\n",
    "            target =torch.eye(self.NUM_CLASSES).index_select(dim=0, index=target)          \n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = data.to(device), target.to(device)#.cuda()\n",
    "\n",
    "            target_m = []\n",
    "            for i in range(len(target)):\n",
    "                n_loc = (target[i,:] == 1).nonzero()\n",
    "                m = torch.zeros(self.NUM_CLASSES,self.NUM_CLASSES)\n",
    "                m[n_loc,n_loc] = 1\n",
    "                target_m.append(m) \n",
    "            target_m = torch.stack(target_m).to(device)  \n",
    "            output_digit, _,masked, output_fc = model(data)  \n",
    "            if self.FC or self.CV: output = output_fc.view(output_fc.size(0),self.NUM_CLASSES,-1)\n",
    "            else: output = output_digit          \n",
    "            #########NEWLOSS########\n",
    "            L_angle = criterionNew.arc_loss(output.squeeze(),target_m,val=1)\n",
    "\n",
    "            b = []\n",
    "            for i in range(len(L_angle)):\n",
    "                b.append(torch.diag(L_angle[i]))\n",
    "            b = torch.stack(b)\n",
    "\n",
    "            _,label = torch.max(target, 1)\n",
    "            loss_AN =criterion(b,label.long())\n",
    "            if self.lossML:\n",
    "                if self.onlyML:\n",
    "                    loss_ML = criterionNew.margin_loss(output_digit, target)\n",
    "                elif self.ML_REC:\n",
    "                    loss_ML = criterionNew.loss(data,output_digit,target,recostruction) \n",
    "                elif self.onlyREC:\n",
    "                    loss_ML = criterionNew.reconstruction_loss(data,recostruction)\n",
    "            else:\n",
    "                loss_ML = 0.0\n",
    "            loss = self.compute_loss(loss_AN,loss_ML)\n",
    "            test_loss += float(loss.data)\n",
    "            test_accuracy += float(sum(np.argmax(b.data.cpu().numpy(), 1) == np.argmax(target.data.cpu().numpy(), 1))) / float(data.size(0))\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                    print(\"BATCH_{}: test accuracy:\".format(batch_id, sum(np.argmax(b.data.cpu().numpy(), 1) == \n",
    "                                           np.argmax(target.data.cpu().numpy(), 1)) / float(data.size(0)) ))    \n",
    "                    print(\"BATCH_{}:loss {} margin loss {}\" .format(batch_id,loss.data,loss_ML))\n",
    "        end = time.time()  \n",
    "        print(\"TEST: Validation time execution {}\".format(end-start))\n",
    "        print(\"TEST: Loss value for test phase: {}\".format(test_loss / len(self.dataloaders['val'])))\n",
    "        print(\"TEST: Accuracy value for test phase: {}\".format(test_accuracy / len(self.dataloaders['val'])))\n",
    "        if self.save_files:\n",
    "            with open(self.implementation_folder_name+\"TEST.txt\", \"a\") as text_file:\n",
    "                text_file.write(\"Validation time execution {}\\n\".format(end-start))\n",
    "                text_file.write(\"Loss value for test phase: {}\\n\".format(test_loss / len(self.dataloaders['val'])))\n",
    "                text_file.write(\"Accuracy value for test phase: {}\\n\".format(test_accuracy / len(self.dataloaders['val'])))\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
